# Segmental DAM pytorch implementation
This project contains code, hyper-parameters and serialized models for the open source implementation of different BoW attention models.
The full description is available here: TBA

If you use this software for academic research please cite the described paper.

# Requirements
- Python 2
- Python 3
- Pytorch

# Usage
Download the datasets and run the preprocessing scripts (Preprocess_scripts folder):

```

```

Launch the models:
```

```
# Demo
There is a demo file inside the code directory to show how models are load to replicate the results on the SNLI dataset:
```

```

# Serialized models
Models can be downloaded from google drive (total of 1.2G)

- DAM BoW: 
- DAM CNN: 
- DAM REC: 
- DAM Seg: 

# Acknowledgements

The project is motivated by the following paper and github repositories:

* A. Parikh, O. Täckström, D. Das, J. Uszkoreit, A decomposable attention model for natural language inference, in: Proceedings of the 2016 Conference on Empirical Methods in Natural Language Process
ing, Association for Computational Linguistics, Austin, Texas, 2016, pp. 2249–2255. URL https://aclweb.org/anthology/D16-1244
* Decomposable Attention Model for Sentence Pair Classification, https://github.com/harvardnlp/decomp-attn
* SNLI-decomposable-attention, https://github.com/libowen2121/SNLI-decomposable-attention
* decomposable_attention, https://github.com/shuuki4/decomposable_attention
